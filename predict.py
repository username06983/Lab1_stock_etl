# -*- coding: utf-8 -*-
"""predict.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Uai_EAPBw93nvWy9CHJ8MJ5yeTVKozA6
"""

from datetime import timedelta, timezone

from airflow import DAG
from airflow.models import Variable
from airflow.decorators import task

from datetime import timedelta
from datetime import datetime
from airflow.utils.dates import days_ago
from airflow.hooks.base import BaseHook
from airflow.utils.state import TaskInstanceState
from airflow.providers.snowflake.hooks.snowflake import SnowflakeHook
from airflow.sensors.external_task import ExternalTaskSensor
from airflow.utils.state import DagRunState

from airflow.utils.session import provide_session
from airflow.models.dagrun import DagRun
from airflow.utils.state import DagRunState
#from your_module import train, predict, return_snowflake_conn

def return_snowflake_conn():

    hook = SnowflakeHook(snowflake_conn_id='snowflake_conn')

    conn = hook.get_conn()
    return conn.cursor()

@task
def train(db, schema, forecast_function):
    cur = return_snowflake_conn()
    base_tbl        = f"{db}.{schema}.STOCK_PRICE"
    train_view = f"{db}.{schema}.STOCK_PRICE_TRAIN_VW"
    forecast_fn = f"{db}.{schema}.{forecast_function}"

    create_view_sql = f"""
    CREATE OR REPLACE VIEW {train_view} AS
    SELECT SYMBOL, DATE, CLOSE
    FROM {base_tbl}
    WHERE DATE >= DATEADD('day', -180, CURRENT_DATE());
    """

    create_model_sql = f"""
    CREATE OR REPLACE SNOWFLAKE.ML.FORECAST {forecast_fn} (
        INPUT_DATA        => SYSTEM$REFERENCE('VIEW', '{train_view}'),
        SERIES_COLNAME    => 'SYMBOL',
        TIMESTAMP_COLNAME => 'DATE',
        TARGET_COLNAME    => 'CLOSE',
        CONFIG_OBJECT     => OBJECT_CONSTRUCT('on_error','skip','evaluate',true)
    );
    """

    cur.execute(create_view_sql)
    cur.execute(create_model_sql)
    cur.execute(f"CALL {forecast_fn}!SHOW_EVALUATION_METRICS();")

@task
def predict(db, schema, forecast_function_name,
            train_input_table, forecast_table, final_table):

    cur = return_snowflake_conn()
    forecast_fn  = f"{db}.{schema}.{forecast_function_name}"
    train_tbl    = f"{db}.{schema}.{train_input_table}"
    forecast_tbl = f"{db}.{schema}.{forecast_table}"
    final_tbl    = f"{db}.{schema}.{final_table}"

    make_prediction_sql = f"""BEGIN
        CALL {forecast_fn}!FORECAST(
            FORECASTING_PERIODS => 7,
            CONFIG_OBJECT => OBJECT_CONSTRUCT('prediction_interval', 0.95, 'on_error', 'skip')
        );
        LET x := SQLID;
        CREATE OR REPLACE TABLE {forecast_tbl} AS
            SELECT
                REPLACE(series, '\"', '')::VARCHAR AS symbol,
                CAST(ts AS DATE) AS date,
                forecast::FLOAT AS forecast,
                lower_bound::FLOAT AS lower_bound,
                upper_bound::FLOAT AS upper_bound
            FROM TABLE(RESULT_SCAN(:x));
    END;"""

    create_final_table_sql = f"""CREATE OR REPLACE TABLE {final_tbl} (
        symbol VARCHAR NOT NULL,
        date DATE NOT NULL,
        actual FLOAT,
        forecast FLOAT,
        lower_bound FLOAT,
        upper_bound FLOAT,
        PRIMARY KEY (symbol, date)
    );

    INSERT INTO {final_tbl} (symbol, date, actual, forecast, lower_bound, upper_bound)
    SELECT SYMBOL, DATE, CLOSE AS actual,
          NULL::FLOAT AS forecast, NULL::FLOAT AS lower_bound, NULL::FLOAT AS upper_bound
    FROM {train_tbl}
    UNION ALL
    SELECT symbol, date, NULL::FLOAT AS actual,
          forecast, lower_bound, upper_bound
    FROM {forecast_tbl};
    """

@provide_session
def latest_successful_etl_run(execution_date, session=None, **_):
    dr = (
        session.query(DagRun)
        .filter(
            DagRun.dag_id == "Stocks_data_ETL",
            DagRun.state == DagRunState.SUCCESS,
        )
        .order_by(DagRun.execution_date.desc())
        .first()
    )
    return dr.execution_date if dr else execution_date

with DAG(
  dag_id="TrainPredict",
  start_date=days_ago(1),
  catchup=False,
  tags=["ML", "ELT"],
  schedule=None,
  ) as dag:
  extras = (BaseHook.get_connection("snowflake_conn").extra_dejson or {})
  db = extras.get("database")
  schema = "STOCKS"

  train_input_table      = "STOCK_PRICE"
  forecast_table         = "STOCK_PRICE_FORECAST_7D"
  forecast_function_name = "PREDICT_STOCK_PRICE"
  final_table            = "STOCK_PRICE_PLUS_FORECAST"


  train_task = train(db, schema, forecast_function_name)
  predict_task = predict(db, schema,
                        forecast_function_name,
                        train_input_table, forecast_table, final_table)

  # Dependency chain: ETL -> train -> predict
  train_task >> predict_task